# BiLSTM-Attention模型评估报告

## 1. 模型架构

### 1.1 整体架构
- 基础模型：双向LSTM（BiLSTM）
- 注意力机制：自注意力层
- 词嵌入维度：300
- 隐藏层维度：128
- 输出层：Sigmoid激活函数

### 1.2 主要组件
- 词嵌入层：将输入文本转换为密集向量表示
- BiLSTM层：捕获文本的双向上下文信息
- 注意力层：自适应加权重要特征
- 全连接层：映射到18个方面类别

## 2. 训练配置

### 2.1 数据处理
- 最大序列长度：128
- 词表构建：使用jieba分词，过滤低频词
- 特殊标记：`<PAD>`和`<UNK>`

### 2.2 训练参数
- 批次大小：32
- 优化器：Adam
- 损失函数：二元交叉熵（BCELoss）
- 训练轮数：10
- 随机种子：42

## 3. 性能评估

### 3.1 评估指标
- 准确率（ACC）：衡量整体预测准确性
- 精确率（PPV）：衡量正例预测的准确性
- 召回率（TPR）：衡量正例识别的完整性
- 特异度（TNR）：衡量负例识别的准确性
- F1分数：精确率和召回率的调和平均

### 3.2 可视化分析
- metrics_curves.png：展示各项指标随训练轮数的变化趋势
- training_curves.png：展示训练损失和验证损失的变化

### 3.3 细粒度评估
对18个不同方面类别进行独立评估，包括：
- 位置相关：交通、市中心位置、易找程度
- 服务相关：排队、服务态度、停车、及时性
- 价格相关：价格水平、性价比、折扣
- 环境相关：装修、噪音、空间、卫生
- 食物相关：分量、口味、外观、推荐度

## 4. 模型优势

1. 双向特征提取
   - BiLSTM能够捕获文本的前向和后向上下文信息
   - 提高了模型对长距离依赖的理解能力

2. 注意力机制
   - 自适应关注重要特征
   - 提高了模型对关键信息的识别能力

3. 多标签分类
   - 同时预测18个方面的情感极性
   - 保持了各个方面之间的关联性

## 5. 改进方向

1. 模型架构优化
   - 考虑引入预训练语言模型（如BERT）
   - 探索更复杂的注意力机制

2. 训练策略改进
   - 实现学习率调度
   - 添加正则化技术
   - 使用更多的数据增强方法

3. 特征工程
   - 引入额外的语言特征
   - 考虑词性和依存关系信息

## 6. 结论

BiLSTM-Attention模型在餐饮评论细粒度情感分析任务上展现出良好的性能。通过双向LSTM和注意力机制的结合，模型能够有效捕获文本中的情感特征，并对不同方面进行准确的情感极性判断。可视化分析显示模型训练过程稳定，各项指标均达到较好水平。未来可以通过优化模型架构、改进训练策略等方式进一步提升模型性能。